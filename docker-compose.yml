# Docker Compose for local development
# Usage: docker-compose up -d

services:
  # Arandu backend + frontend
  arandu:
    build: .
    ports:
      - "8080:8080"
    environment:
      - PORT=8080
      - DATABASE_URL=/data/database.db
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2.5-coder:14b}
      - OLLAMA_SERVER_URL=http://ollama:11434
      - CORS_ALLOWED_ORIGINS=http://localhost:8080,http://localhost:5173
      - ALLOW_ANY_DOCKER_IMAGE=true
      - LOG_LEVEL=info
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - arandu-data:/data
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/playground"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Ollama for local LLM inference
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    # Uncomment for GPU support (NVIDIA)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # Development mode: run backend and frontend separately
  backend-dev:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    ports:
      - "8080:8080"
    environment:
      - PORT=8080
      - DATABASE_URL=database.db
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2.5-coder:14b}
      - OLLAMA_SERVER_URL=http://ollama:11434
      - CORS_ALLOWED_ORIGINS=http://localhost:5173
      - ALLOW_ANY_DOCKER_IMAGE=true
      - LOG_LEVEL=debug
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - ./backend:/app
      - /var/run/docker.sock:/var/run/docker.sock
    working_dir: /app
    command: go run .
    profiles:
      - dev

  frontend-dev:
    image: node:22-slim
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=localhost:8080
    volumes:
      - ./frontend:/app
      - ./backend/graph/schema.graphqls:/backend/graph/schema.graphqls
      - frontend-node-modules:/app/node_modules
    working_dir: /app
    command: sh -c "yarn install && yarn dev --host 0.0.0.0"
    profiles:
      - dev

volumes:
  arandu-data:
  ollama-models:
  frontend-node-modules:
