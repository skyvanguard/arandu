package providers

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"

	"github.com/arandu-ai/arandu/assets"
	"github.com/arandu-ai/arandu/database"
	"github.com/arandu-ai/arandu/logging"
	"github.com/arandu-ai/arandu/models"
	"github.com/arandu-ai/arandu/security"
	"github.com/arandu-ai/arandu/templates"

	"github.com/invopop/jsonschema"
	"github.com/tmc/langchaingo/llms"
)

type ProviderType string

const (
	ProviderOpenAI           ProviderType = "openai"
	ProviderOllama           ProviderType = "ollama"
	ProviderLMStudio         ProviderType = "lmstudio"
	ProviderLocalAI          ProviderType = "localai"
	ProviderOpenAICompatible ProviderType = "openai-compatible"
)

// Constantes de truncado de prompts
const (
	// DefaultMaxPromptLength es el tamaño máximo del prompt en caracteres
	DefaultMaxPromptLength = 30000
	// ModerateTruncateLength es el límite para truncado moderado de resultados
	ModerateTruncateLength = 2000
	// AggressiveTruncateLength es el límite para truncado agresivo de resultados
	AggressiveTruncateLength = 500
)

type Provider interface {
	New() Provider
	Name() ProviderType
	Summary(query string, n int) (string, error)
	DockerImageName(task string) (string, error)
	NextTask(args NextTaskOptions) *database.Task
}

type NextTaskOptions struct {
	Tasks       []database.Task
	DockerImage string
}

var Tools = []llms.Tool{
	{
		Type: "function",
		Function: &llms.FunctionDefinition{
			Name:        "terminal",
			Description: "Calls a terminal command",
			Parameters:  jsonschema.Reflect(&TerminalArgs{}).Definitions["TerminalArgs"],
		},
	},
	{
		Type: "function",
		Function: &llms.FunctionDefinition{
			Name:        "browser",
			Description: "Opens a browser to look for additional information",
			Parameters:  jsonschema.Reflect(&BrowserArgs{}).Definitions["BrowserArgs"],
		},
	},
	{
		Type: "function",
		Function: &llms.FunctionDefinition{
			Name:        "code",
			Description: "Modifies or reads code files",
			Parameters:  jsonschema.Reflect(&CodeArgs{}).Definitions["CodeArgs"],
		},
	},
	{
		Type: "function",
		Function: &llms.FunctionDefinition{
			Name:        "ask",
			Description: "Sends a question to the user for additional information",
			Parameters:  jsonschema.Reflect(&AskArgs{}).Definitions["AskArgs"],
		},
	},
	{
		Type: "function",
		Function: &llms.FunctionDefinition{
			Name:        "done",
			Description: "Mark the whole task as done. Should be called at the very end when everything is completed",
			Parameters:  jsonschema.Reflect(&DoneArgs{}).Definitions["DoneArgs"],
		},
	},
}

func ProviderFactory(provider ProviderType) (Provider, error) {
	switch provider {
	case ProviderOpenAI:
		return OpenAIProvider{}.New(), nil
	case ProviderOllama:
		return OllamaProvider{}.New(), nil
	case ProviderLMStudio:
		return LMStudioProvider{}.New(), nil
	case ProviderLocalAI:
		return LocalAIProvider{}.New(), nil
	case ProviderOpenAICompatible:
		return OpenAICompatibleProvider{}.New(), nil
	default:
		return nil, fmt.Errorf("unknown provider: %s. Available: openai, ollama, lmstudio, localai, openai-compatible", provider)
	}
}

func defaultAskTask(message string) *database.Task {
	task := database.Task{
		Type: database.StringToNullString("ask"),
	}

	task.Args = database.StringToNullString("{}")
	task.Message = sql.NullString{
		String: fmt.Sprintf("%s. What should I do next?", message),
		Valid:  true,
	}

	return &task
}

func tasksToMessages(tasks []database.Task, prompt string) []llms.MessageContent {
	var messages []llms.MessageContent
	messages = append(messages, llms.MessageContent{
		Role: llms.ChatMessageTypeSystem,
		Parts: []llms.ContentPart{
			llms.TextPart(prompt),
		},
	})

	for _, task := range tasks {
		if task.Type.String == "input" {
			messages = append(messages, llms.MessageContent{
				Role: llms.ChatMessageTypeHuman,
				Parts: []llms.ContentPart{
					llms.TextPart(prompt),
				},
			})
		}

		if task.ToolCallID.String != "" {
			messages = append(messages, llms.MessageContent{
				Role: llms.ChatMessageTypeAI,
				Parts: []llms.ContentPart{
					llms.ToolCall{
						ID: task.ToolCallID.String,
						FunctionCall: &llms.FunctionCall{
							Name:      task.Type.String,
							Arguments: task.Args.String,
						},
						Type: "function",
					},
				},
			})

			messages = append(messages, llms.MessageContent{
				Role: llms.ChatMessageTypeTool,
				Parts: []llms.ContentPart{
					llms.ToolCallResponse{
						ToolCallID: task.ToolCallID.String,
						Name:       task.Type.String,
						Content:    task.Results.String,
					},
				},
			})
		}

		// This Ask was generated by the agent itself in case of some error (not the OpenAI)
		if task.Type.String == "ask" && task.ToolCallID.String == "" {
			messages = append(messages, llms.MessageContent{
				Role: llms.ChatMessageTypeAI,
				Parts: []llms.ContentPart{
					llms.TextPart(task.Message.String),
				},
			})
		}
	}

	return messages
}

func textToTask(text string) (*database.Task, error) {
	c := unmarshalCall(text)

	if c == nil {
		return nil, fmt.Errorf("can't unmarshalCall %s", text)
	}

	task := database.Task{
		// TODO validate tool name
		Type: database.StringToNullString(c.Tool),
	}

	arg, err := json.Marshal(c.Input)
	if err != nil {
		logging.Error("Failed to marshal terminal args", "error", err.Error())
		return defaultAskTask("There was an error running the terminal command"), nil
	}
	task.Args = database.StringToNullString(string(arg))

	// Sometimes the model returns an empty string for the message
	// In that case, we use the input as the message
	msg := c.Message
	if msg == "" {
		msg = string(arg)
	}

	task.Message = database.StringToNullString(msg)
	task.Status = database.StringToNullString(models.TaskInProgress)

	return &task, nil
}

func unmarshalCall(input string) *Call {
	// Security: Sanitize log output to prevent sensitive data exposure
	sanitizedInput := security.SanitizeLogMessage(input)
	logging.Debug("Unmarshalling tool call", "input", sanitizedInput)

	var c Call

	err := json.Unmarshal([]byte(input), &c)
	if err != nil {
		logging.Warn("Failed to unmarshal tool call", "error", err.Error())
		return nil
	}

	if c.Tool != "" {
		logging.Debug("Unmarshalled tool call", "tool", c.Tool)
		return &c
	}

	return nil
}

func toolToTask(choices []*llms.ContentChoice) (*database.Task, error) {
	if len(choices) == 0 {
		return nil, fmt.Errorf("no choices found, asking user")
	}

	toolCalls := choices[0].ToolCalls

	if len(toolCalls) == 0 {
		return nil, fmt.Errorf("no tool calls found, asking user")
	}

	tool := toolCalls[0]

	task := database.Task{
		Type: database.StringToNullString(tool.FunctionCall.Name),
	}

	if tool.FunctionCall.Name == "" {
		return nil, fmt.Errorf("no tool name found, asking user")
	}

	// We use AskArgs to extract the message
	var toolType Messanger

	switch tool.FunctionCall.Name {
	case "input":
		toolType = &InputArgs{}
	case "terminal":
		toolType = &TerminalArgs{}
	case "browser":
		toolType = &BrowserArgs{}
	case "code":
		toolType = &CodeArgs{}
	case "ask":
		toolType = &AskArgs{}
	case "done":
		toolType = &DoneArgs{}
	default:
		return nil, fmt.Errorf("unknown tool name: %s", tool.FunctionCall.Name)
	}

	params, err := extractToolArgs(tool.FunctionCall.Arguments, &toolType)
	if err != nil {
		return nil, fmt.Errorf("failed to extract args: %v", err)
	}
	args, err := json.Marshal(params)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal terminal args, asking user: %v", err)
	}
	task.Args = database.StringToNullString(string(args))

	// Sometimes the model returns an empty string for the message
	msg := string((*params).GetMessage())
	if msg == "" {
		msg = tool.FunctionCall.Arguments
	}

	task.Message = database.StringToNullString(msg)
	task.Status = database.StringToNullString(models.TaskInProgress)

	task.ToolCallID = database.StringToNullString(tool.ID)

	return &task, nil
}

func extractToolArgs[T any](functionArgs string, args *T) (*T, error) {
	err := json.Unmarshal([]byte(functionArgs), args)
	if err != nil {
		return nil, fmt.Errorf("failed to unmarshal args: %v", err)
	}
	return args, nil
}

// truncateTasks reduces the size of task history when the prompt is too long (Bug fix #73)
// It keeps the first few and last few tasks intact, and truncates results of middle tasks
func truncateTasks(tasks []database.Task, maxResultLength int) []database.Task {
	if len(tasks) <= 6 {
		// If we have few tasks, just truncate long results
		return truncateTaskResults(tasks, maxResultLength)
	}

	// Keep first 3 and last 3 tasks intact, truncate middle ones more aggressively
	truncated := make([]database.Task, len(tasks))
	copy(truncated, tasks)

	for i := range truncated {
		maxLen := maxResultLength
		// Middle tasks get more aggressive truncation
		if i >= 3 && i < len(truncated)-3 {
			maxLen = maxResultLength / 4
		}

		if truncated[i].Results.Valid && len(truncated[i].Results.String) > maxLen {
			truncated[i].Results.String = truncated[i].Results.String[:maxLen] + "\n... [truncated]"
		}
	}

	return truncated
}

// truncateTaskResults truncates long results in all tasks
func truncateTaskResults(tasks []database.Task, maxLength int) []database.Task {
	truncated := make([]database.Task, len(tasks))
	copy(truncated, tasks)

	for i := range truncated {
		if truncated[i].Results.Valid && len(truncated[i].Results.String) > maxLength {
			truncated[i].Results.String = truncated[i].Results.String[:maxLength] + "\n... [truncated]"
		}
	}

	return truncated
}

// PromptConfig contains options for preparing a prompt
type PromptConfig struct {
	DockerImage     string
	Tasks           []database.Task
	UseToolCalls    bool
	MaxPromptLength int
}

// PreparedPrompt contains the rendered prompt and messages ready for the LLM
type PreparedPrompt struct {
	Prompt   string
	Messages []llms.MessageContent
	Tasks    []database.Task
}

// PreparePrompt prepares the prompt with truncation if needed
// This is shared logic used by all providers
func PreparePrompt(cfg PromptConfig) (*PreparedPrompt, error) {
	if cfg.MaxPromptLength == 0 {
		cfg.MaxPromptLength = DefaultMaxPromptLength
	}

	tasks := cfg.Tasks

	var toolPlaceholder string
	if cfg.UseToolCalls {
		toolPlaceholder = "Always use your function calling functionality, instead of returning a text result."
	} else {
		toolPlaceholder = getToolPlaceholder()
	}

	promptArgs := map[string]interface{}{
		"DockerImage":     cfg.DockerImage,
		"ToolPlaceholder": toolPlaceholder,
		"Tasks":           tasks,
	}

	prompt, err := renderPrompt(promptArgs)
	if err != nil {
		return nil, fmt.Errorf("failed to render prompt: %w", err)
	}

	// First truncation attempt: moderate
	if len(prompt) > cfg.MaxPromptLength {
		logging.Info("Prompt too long, attempting to truncate task results",
			"current_length", len(prompt),
			"max_length", cfg.MaxPromptLength,
		)
		tasks = truncateTasks(cfg.Tasks, ModerateTruncateLength)
		promptArgs["Tasks"] = tasks
		prompt, err = renderPrompt(promptArgs)
		if err != nil {
			return nil, fmt.Errorf("failed to render truncated prompt: %w", err)
		}
	}

	// Second truncation attempt: aggressive
	if len(prompt) > cfg.MaxPromptLength {
		logging.Warn("Prompt still too long, using aggressive truncation",
			"current_length", len(prompt),
			"max_length", cfg.MaxPromptLength,
		)
		tasks = truncateTasks(cfg.Tasks, AggressiveTruncateLength)
		promptArgs["Tasks"] = tasks
		prompt, err = renderPrompt(promptArgs)
		if err != nil {
			return nil, fmt.Errorf("failed to render aggressively truncated prompt: %w", err)
		}
	}

	// Still too long after all attempts
	if len(prompt) > cfg.MaxPromptLength {
		return nil, fmt.Errorf("prompt too long (%d chars) after truncation", len(prompt))
	}

	return &PreparedPrompt{
		Prompt:   prompt,
		Messages: tasksToMessages(tasks, prompt),
		Tasks:    tasks,
	}, nil
}

// renderPrompt is a helper to render the agent prompt template
func renderPrompt(args map[string]interface{}) (string, error) {
	return templates.Render(assets.PromptTemplates, "prompts/agent.tmpl", args)
}

// LLMClient is an interface for LLM clients that support GenerateContent
type LLMClient interface {
	GenerateContent(ctx context.Context, messages []llms.MessageContent, options ...llms.CallOption) (*llms.ContentResponse, error)
}

// GenerateTaskConfig contains options for generating the next task
type GenerateTaskConfig struct {
	Client       LLMClient
	Model        string
	Messages     []llms.MessageContent
	UseToolCalls bool
	Temperature  float64
	TopP         float64
}

// GenerateNextTask generates the next task from an LLM response
// This is shared logic used by all providers
func GenerateNextTask(ctx context.Context, cfg GenerateTaskConfig) (*database.Task, error) {
	if cfg.Temperature == 0 {
		cfg.Temperature = 0.0
	}
	if cfg.TopP == 0 {
		cfg.TopP = 0.2
	}

	opts := []llms.CallOption{
		llms.WithTemperature(cfg.Temperature),
		llms.WithModel(cfg.Model),
		llms.WithTopP(cfg.TopP),
		llms.WithN(1),
	}

	if cfg.UseToolCalls {
		opts = append(opts, llms.WithTools(Tools))
	}

	resp, err := cfg.Client.GenerateContent(ctx, cfg.Messages, opts...)
	if err != nil {
		return nil, fmt.Errorf("failed to get response from model: %w", err)
	}

	if resp == nil {
		return nil, fmt.Errorf("received nil response from model")
	}

	// Try to parse as tool call first
	if cfg.UseToolCalls && len(resp.Choices) > 0 && len(resp.Choices[0].ToolCalls) > 0 {
		task, err := toolToTask(resp.Choices)
		if err == nil {
			return task, nil
		}
		logging.Debug("Failed to parse tool call, trying text", "error", err.Error())
	}

	// Fall back to text/JSON parsing
	if len(resp.Choices) > 0 && resp.Choices[0].Content != "" {
		task, err := textToTask(resp.Choices[0].Content)
		if err == nil {
			return task, nil
		}
		logging.Debug("Failed to parse text response", "error", err.Error())
	}

	return nil, fmt.Errorf("couldn't parse model response")
}
